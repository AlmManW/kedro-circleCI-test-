import pandas as pd
'''
import findspark
findspark.init()

from pyspark.sql.functions import lpad, collect_set, collect_list, year, month, dayofmonth, concat_ws, when, concat, count, max, lit, countDistinct, size, split, lower, col, substring, length, udf, format_string, date_format, to_utc_timestamp
import pyspark.sql.functions as f
from pyspark.sql.types import StringType, FloatType, ArrayType, IntegerType, DoubleType
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import HiveContext
import pyspark

print("Crear contexto de spark") 
spark = SparkSession.builder.config("spark.driver.memory", "6g").config("spark.hadoop.fs.s3a.access.key", "AKIAZUZ7CBT7A6MQZ6XD").config("spark.hadoop.fs.s3a.secret.key", "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").getOrCreate()
print("Lectura s3")
df = spark.read.parquet("s3a://sumz-kobaqa-bucket/BCT_KOBA/raw/trx_header")
df.printSchema()
'''
