# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html
csv_data_read:
  type: spark.SparkDataSet
  file_format: csv
  load_args: [["header", "True"]]
  filepath: C:\Users\William\Documents\Sumz\KOBA-D1\Scorpius\local_scorpius\data\01_raw\TRX_HEADER_202111090016_Nov02.csv

csv_data_load:
  type: spark.SparkDataSet
  file_format: parquet
  filepath:  C:\Users\William\Documents\Sumz\KOBA-D1\Scorpius\local_scorpius\data\01_raw\reviews.parquet


#trx_header_aws_parquet:
#  type: spark.SparkDataSet
#  file_format: parquet
#  #filepath: /data/app_sumz/enviroments/scorpius/data/02_intermediate/TRX_HEADER_202111021542.csv
#  filepath: s3a://sumz-kobaqa-bucket/trx_header.parquet
#  credentials: key_access_s3
